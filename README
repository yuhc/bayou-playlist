# Bayou

This is a Bayou implementation based on D. Terry's Managing Update Conflicts
in Bayou [1], and Session Guarantees for Weakly Consistent Replicated Data [2],
and K. Petersen's Flexible Update Propagation [3].

Now the README is copied directly from Paxos project. It will be updated later.


Project Information
==============================================================================
CS 380D Distributed Computing, Fall 2015

Project 3, Bayou
* Hangchen Yu, UT EID: HY4987, UTCS ID: ZHITINGZ
* Zhiting Zhu, UT EID: ZZ3883, UTCS ID: HYU


Testing the Project
==============================================================================
The source codes are put in `./src`. `COMMAND` file contains a single line
`./src/Master.py` which starts the master of Paxos system. Execute:
```
$ ./tester.py
```

It will test all the input files in `./tests`. The inputs must be named with
extension `.test`. It will compare the output of `Master.py` with the standard
outputs in `./solutions`.

`Python3` is required.

We supply **34** test cases including the original ones. They cover most special
cases including the basic operations and the session guarantees.

Some Assumptions
==============================================================================
* The tester should leave enough minutes for each test case. For the given test
cases, our program runs in less than 5 min (which is the time given by the
default tester). But it may run longer when given other tests (e.g. creating
20 servers and executing a `stabilize).

* No crashes in the system.

* A client will only connect one server at a time.

* You should make sure that the indexes are not out of bound (otherwise that
command will be ignored).

* The channel is valid.

* `retirement` should be passed via Anti-Entropy.


Modification of Bayou
==============================================================================

Master
------------------------------------------------------------------------------
The master reads the input commands, and send messages to clients or servers
accordingly. Besides, the master needs to create the processes of servers and
clients. All the output that tester needs are written by master (to stdout).

As there are some commands requiring blocking, we use either asynchronous
blocking (conditional locks) or simple `time.sleep()` to wait for the completion
of these commands. At the end of the execution of master, it kills all the
processes in order to release the ports occupied by them.

Server
------------------------------------------------------------------------------
The code for server lives in server.py. Each server has the attributes of its
`node_id`, `unique_id`, `accept_time` (of writes), `version_vector` etc. Some
technical details are stated in the Technique Section.

The server creates a thread to handle the socket messages. It also creates a
separate thread to handle each anti-entropy process. We use conditional locks to
guarantee that the database is being edited only by one process.

The functions of server are mainly based on [1] and [3].

Client
------------------------------------------------------------------------------
The client sends messages to all replicas (numbered from `0`..`f`) in the
system. It stores the messages in a buffer, and remove the message from the
buffer when receiving `decision` of it. When the buffer is empty, the client
tells the master that the blocking of `allClear` can be cleared. When a client
hears that the election of new leader is completed, it will resend the messages
in the buffer.

According to the implementation, when client does not clear its message buffer
after long enough time, the system is stable (not changed meaningfully by the
time). Then client will return `allClear` to master directly, and move the
progress on.

In order to deal with the case where a sendMessage is executed when all
replicas fails, the client will notify the leader to reset the state
periodically when it is dealing with `allClear` from master. This operation
is aimed to reset the whole Paxos system.


Some Techniques
==============================================================================

Message Formats
------------------------------------------------------------------------------
The messages are stored as objects. They are defined in `./src/message.py`.
There are three kinds of messages,

Trigger Anti-Entropy
------------------------------------------------------------------------------
The Anti-Entropy process described in [3] is triggered periodically. The period
is randomly selected at the end of each Anti-Entropy. The server keeps four
lists: `server_list`, `block_list`, `sent_list` and `available_list`. Each time
the server will choose one destination from `available_list` to request the
Anti-Entropy.

The Anti-Entropy can either be accepted or be rejected by the destination. This
is to guarantee that the database will not be modified by several instances.
The destination will try the lock to determine whether it accepts the request.



Failure Detection
------------------------------------------------------------------------------
The only leader in the system keeps broadcasting heartbeat to other servers.
Each server creates a thread to detect the heartbeat. When a server does not
receive heartbeat in a fixed period (timeout), this server regards the leader
crashed, and starts a leader election protocol.

Leader Election
------------------------------------------------------------------------------
We made a bit modification of the leader election in Three-Phase-Commit project.
When a leader election starts, the server marks `leader_is_dead` as `True`,
regards itself as the new leader, and broadcast a heartbeat containing its
`node_id`. If a server receives a heartbeat from others, it updates its
`leader_id` to the `node_id` in the heartbeat when `node_id` smaller than its
own id or `leader_is_dead` is `True`. Then it marks `leader_is_dead` as `False`.

After three rounds (actually two are enough), all servers have the same
`leader_id`, which is the smallest index of them. Then this new leader sends a
acknowledge message to all Clients. The Clients then resend the
messages in their waiting queues.


Using Auto Grader
==============================================================================
Run "python tester.py" and check your output.

To add your own tests, create a .test file and a .sol file in the tests/ and solutions/ directories respectively.
You can check what your output was in the answers/ directory.

Running tests manually:

To run the sample tests, replace test_name with the name of the test and execute the following command:

cat tests/[test_name].test | $(cat COMMAND)

To automatically check the output:

cat tests/[test_name].test | $(cat COMMAND) > temp_output && diff -q temp_output tests/[test_name].sol

If your output matches the solution, NOTHING will be printed. Otherwise the lines that differ will be shown.
The output for the run of the test will also be stored in a file temp_output after running the second command.
